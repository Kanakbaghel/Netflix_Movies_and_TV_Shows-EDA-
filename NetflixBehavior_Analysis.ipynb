{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "*I‚Äôve spent countless hours watching Netflix... so you don‚Äôt have to.*\n",
        "\n",
        "## **Why Do We Keep Clicking \"Next Episode\"?**\n",
        "\n",
        "**What makes a show so addictive that we just can‚Äôt stop watching?**  \n",
        "For engineers and content strategists, this question drives ongoing research into viewer behavior and engagement design.\n",
        "\n",
        "From gripping cliffhangers to smart pacing, some shows seem built to pull us in‚Äîuntil it‚Äôs 2 AM and we‚Äôre whispering: **just one more episode**.\n",
        "\n",
        "**What keeps us coming back, episode after episode, even when we know we should stop?**\n",
        "\n",
        "This project explores that question using data from over **8,800 Netflix titles**, uncovering the patterns behind binge-worthy content.\n",
        "\n",
        "### What‚Äôs inside the notebook:\n",
        "- What makes a show easy to binge‚Äîlike pacing, episode length, and release format  \n",
        "- A simple machine learning model that predicts a show‚Äôs ‚ÄúBinge Score‚Äù  \n",
        "- Which genres use cliffhangers most effectively  \n",
        "- Tips for creators and platforms to design content that keeps viewers engaged  \n",
        "- How binge habits vary across different countries\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NBweY16pS501"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "===============================\n",
        "# üîπ IMPORT LIBRARIES\n",
        "==============================="
      ],
      "metadata": {
        "id": "5h5T88GLRwu6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "UO86--VLHcq2",
        "outputId": "d635f8f4-7019-4163-a011-3c24333fcb8c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy>=1.22.0 (from scikit-learn)\n",
            "  Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Using cached scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "Using cached scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.2\n",
            "    Uninstalling numpy-2.3.2:\n",
            "      Successfully uninstalled numpy-2.3.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.7.1\n",
            "    Uninstalling scikit-learn-1.7.1:\n",
            "      Successfully uninstalled scikit-learn-1.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.2 numpy-2.3.2 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "scipy",
                  "sklearn",
                  "threadpoolctl"
                ]
              },
              "id": "766f8f45a9cc4c50b3ba63edc800ecab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded Successfully\n"
          ]
        }
      ],
      "source": [
        "## SUPPRESS WARNINGS FOR CLEANER OUTPUT\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "## CORE LIBRARIES\n",
        "import numpy as np  #for numerical operation\n",
        "import pandas as pd #for data handling and analysis\n",
        "\n",
        "## VISUALIZATION LIBRARIES\n",
        "import matplotlib.pyplot as plt #for plots\n",
        "import seaborn as sns #for statistical visualisation\n",
        "\n",
        "## INTERACTIVE PLOTTING WITH PLOTLY\n",
        "import plotly.express as px #high level interface for interactive plots\n",
        "import plotly.graph_objects as go #low level interface for custom visualization\n",
        "from plotly.subplots import make_subplots #for subplots layouts\n",
        "import plotly.offline as pyo #offline mode for plotly in notebook\n",
        "pyo.init_notebook_mode(connected=True) #initialize plotly for jupyter notebook\n",
        "\n",
        "## NLP AND TEXT PREPROCESSING LIBRARIES\n",
        "import re #regular expressions for text cleaning\n",
        "import nltk #natural language toolkit\n",
        "from textblob import TextBlob #for basic sentiment and basic analysis\n",
        "from wordcloud import WordCloud #for generating word clouds\n",
        "\n",
        "# Reinstall scikit-learn cleanly\n",
        "!pip install --upgrade --force-reinstall scikit-learn\n",
        "\n",
        "## FEATURE ENGINEERING AND PREPROCESSING\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler #for scaling and label encoding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #for text vectorization\n",
        "from sklearn.decomposition import PCA  #dimentionality reduction\n",
        "from sklearn.cluster import KMeans #for clustering\n",
        "\n",
        "## MACHINE LEARNING MODELS\n",
        "from sklearn.ensemble import RandomForestRegressor #Regression model\n",
        "\n",
        "## MODEL TRAINING AND EVALUATION\n",
        "from sklearn.model_selection import train_test_split #for splitting data\n",
        "from sklearn.metrics import mean_squared_error, r2_score #for model evaluation\n",
        "\n",
        "## OWLOAD REQUIRED NLTK DATASETS\n",
        "nltk.download('stopwords', quiet=True) #stopword list\n",
        "nltk.download('vader_lexicon', quiet=True) #lexicon for sentiment analysis\n",
        "nltk.download('punkt', quiet=True) #tokenizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer #for VADER sentiment analyzer\n",
        "\n",
        "## VISUALIZATION SETTINGS FOR CONSISTENT STYLING\n",
        "sns.set_style('whitegrid') #white background with gray grid lines\n",
        "plt.rcParams['figure.figsize'] = (8, 6) #default figure size\n",
        "plt.rcParams['legend.fontsize'] = 12 #fontsize of legend\n",
        "plt.rcParams['figure.titlesize'] = 16 #fontsize of figure title\n",
        "plt.rcParams['figure.titleweight'] = 'bold' #bold font weight for figure title\n",
        "plt.rcParams['font.family'] = 'sans-serif' #default font family\n",
        "plt.rcParams['text.color'] = 'black' #default text color\n",
        "plt.rcParams['axes.labelcolor'] = 'black' #default x and y label color\n",
        "pd.set_option('display.max_columns', None) #display all columns in dataframe\n",
        "pd.set_option('display.max_rows', None) #display all rows in dataframe\n",
        "\n",
        "# Netflix-inspired brand colors (refined for visual clarity)\n",
        "NETFLIX_PRIMARY   = '#E50914'  # Signature red\n",
        "NETFLIX_DARK      = '#1A1A1A'  # Deep black for backgrounds\n",
        "NETFLIX_LIGHT     = '#F4F4F4'  # Soft white for contrast\n",
        "NETFLIX_ACCENT    = '#B20710'  # Rich red accent\n",
        "NETFLIX_MUTED     = '#4A4A4A'  # Neutral gray for labels and borders\n",
        "\n",
        "# Custom color palette for visualizations\n",
        "netflix_palette = [\n",
        "    NETFLIX_PRIMARY,\n",
        "    NETFLIX_ACCENT,\n",
        "    NETFLIX_DARK,\n",
        "    NETFLIX_MUTED,\n",
        "    '#8B0000'  # Optional deep red for variation\n",
        "]\n",
        "\n",
        "# Apply palette to Seaborn\n",
        "sns.set_palette(netflix_palette)\n",
        "\n",
        "print(\"Libraries loaded Successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "============================\n",
        "\n",
        "LOADING AND EXPLORING DATASET\n",
        "\n",
        "============================"
      ],
      "metadata": {
        "id": "qqrX2y0Eaq-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Netflix dataset from CSV\n",
        "df = pd.read_csv('/content/netflix_titles.csv')\n",
        "\n",
        "#getting the number of entries and columns\n",
        "loaded = df.shape[0] #total record\n",
        "titles = df.shape[1] # total columns\n",
        "\n",
        "print(f'The dataset has {loaded} rows and {titles} columns.')\n",
        "\n",
        "# number of movies and shows on netflix\n",
        "movies = df[df['type'] == 'Movie'].shape[0]\n",
        "tv_shows = df[df['type'] == 'TV Show'].shape[0]\n",
        "\n",
        "print(f'The dataset has {movies} movies and {tv_shows} TV shows.')"
      ],
      "metadata": {
        "id": "LEOiRLh5Hoc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b60b89f-b82d-49e9-dbc2-3bee973fdad1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset has 8807 rows and 12 columns.\n",
            "The dataset has 6131 movies and 2676 TV shows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =========================\n",
        "# **THE BINGE HYPOTHESIS**\n",
        "# =========================\n",
        "\n",
        "**Before we begin analyzing the data, let‚Äôs define what typically makes a show ‚Äúbingeable‚Äù:**\n",
        "\n",
        "1. **Cliffhanger Structure**  \n",
        "   - Episodes often end with *unresolved tension* or open questions, *prompting viewers to continue watching*.\n",
        "\n",
        "2. **Emotional Feedback Loop**  \n",
        "   - Fast-paced storytelling and frequent *emotional payoffs* create a sense of momentum and reward.\n",
        "\n",
        "3. **Cultural Relevance**  \n",
        "   - Shows that are *widely discussed* or *trending tend* to attract viewers who want to stay in the loop.\n",
        "\n",
        "4. **Cognitive Ease**  \n",
        "   - Content that‚Äôs easy to follow and *doesn‚Äôt require deep concentration* is more likely to be consumed in long stretches.\n",
        "\n",
        "5. **Viewer Commitment Effect**  \n",
        "   - The more time a viewer invests in a series, the harder it becomes to disengage‚Äîcreating a *self-reinforcing cycle*.\n",
        "\n",
        "**Now, Let‚Äôs examine whether the data supports these patterns.**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "P7s5aXgO3XP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================\n",
        "\n",
        "DATA CLEANING AND PREPARATION\n",
        "\n",
        "============================="
      ],
      "metadata": {
        "id": "RhMO06Uq1a5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check out missing values\n",
        "missing_count = df.isnull().sum()\n",
        "missing_percentage = ((missing_count / len(df)) * 100).round(2)\n",
        "missing_df = pd.DataFrame({'Missing Count': missing_count, 'Missing Percentage': missing_percentage}).sort_values(by='Missing Count', ascending=False)\n",
        "print(\"\\n Missing Values Analysis:\")\n",
        "print(missing_df[missing_df['Missing Count']>0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tEs1sf5zar2",
        "outputId": "4a6c2596-3e94-4cae-9a78-75d51f355847"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Missing Values Analysis:\n",
            "            Missing Count  Missing Percentage\n",
            "director             2634               29.91\n",
            "country               831                9.44\n",
            "cast                  825                9.37\n",
            "date_added             10                0.11\n",
            "rating                  4                0.05\n",
            "duration                3                0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dc9510c"
      },
      "source": [
        "Now that we have identified the missing values, we will proceed with handling them as part of the data cleaning and preparation process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cf27173",
        "outputId": "35bd2d52-4481-444c-e2dc-fa8161f24a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Handle missing values:\n",
        "# - Fill 'director', 'cast', and 'country' with 'Unknown' as they are categorical and have a significant number of missing values.\n",
        "df['director'].fillna('Unknown', inplace=True)\n",
        "df['cast'].fillna('Unknown', inplace=True)\n",
        "df['country'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# - Drop rows with missing 'date_added', 'rating', and 'duration' as they represent a very small percentage of the data and are critical for analysis.\n",
        "df.dropna(subset=['date_added', 'rating', 'duration'], inplace=True)\n",
        "\n",
        "# Verify that there are no more missing values\n",
        "print(\"\\nMissing Values after handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(f'\\n data cleaned! Final dataset : {df.shape[0]:,} titles')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing Values after handling:\n",
            "show_id         0\n",
            "type            0\n",
            "title           0\n",
            "director        0\n",
            "cast            0\n",
            "country         0\n",
            "date_added      0\n",
            "release_year    0\n",
            "rating          0\n",
            "duration        0\n",
            "listed_in       0\n",
            "description     0\n",
            "dtype: int64\n",
            "\n",
            " data cleaned! Final dataset : 8,790 titles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "====================\n",
        "\n",
        "FEATURE ENGINEERING\n",
        "\n",
        "===================="
      ],
      "metadata": {
        "id": "2PDI6irV687x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print('ENGINEERING THE BINGE FACTOR')\n",
        "print(\"=\"*60)\n",
        "\n"
      ],
      "metadata": {
        "id": "3r_oJEPo6RDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
